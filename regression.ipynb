{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/features.csv')\n",
    "cleaned = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Tmporal Variables in Features with Static Variables from Cleaned Data\n",
    "cleaned['time'] = pd.to_datetime(cleaned['time'])\n",
    "cleaned['date'] = cleaned['time'].dt.date\n",
    "\n",
    "numeric_cols = cleaned.select_dtypes(include='number').columns.tolist()\n",
    "aggregated = cleaned.groupby(['id', 'date'])[numeric_cols].mean().reset_index()\n",
    "aggregated = aggregated.rename(columns={'date': 'target_date'})\n",
    "\n",
    "merged_df = pd.merge(features, aggregated, on=['id', 'target_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "appcat_cols = [\n",
    "    'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance',\n",
    "    'appCat.game', 'appCat.office', 'appCat.other', 'appCat.social',\n",
    "    'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather', 'mood'\n",
    "]\n",
    "merged_df = merged_df.drop(columns=[col for col in appcat_cols if col in merged_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_features = [\n",
    "    'mood_day5', 'app_usage_day5', 'screen_day5',\n",
    "    'mood_day4', 'app_usage_day4', 'screen_day4',\n",
    "    'mood_day3', 'app_usage_day3', 'screen_day3',\n",
    "    'mood_day2', 'app_usage_day2', 'screen_day2',\n",
    "    'mood_day1', 'app_usage_day1', 'screen_day1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = temporal_features\n",
    "merged_df = merged_df.dropna(subset=features + ['target_mood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the Dataset using Standard Scaler \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(merged_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = X_scaled.reshape(-1, 5, 3)#Reshape to the number of samples, time steps, feature per step\n",
    "y_seq = merged_df['target_mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_seq, y_seq, groups=merged_df['id']))\n",
    "X_train, X_test = X_seq[train_idx], X_seq[test_idx]\n",
    "y_train, y_test = y_seq[train_idx], y_seq[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   return_sequences=False))\n",
    "    \n",
    "    if hp.Boolean('dropout'):\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='log'))\n",
    "    model.compile(optimizer=optimizer, loss=hp.Choice('loss', ['mean_squared_error', 'mean_absolute_error']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory='tuner_dir',\n",
    "    project_name='lstm_mood_prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32) #Search for Best Model Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_trial.hyperparameters.values) # Best Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = best_model.predict(X_test)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Model Error Metrics :\")\n",
    "print(f\"MSE: {mse_best:0.4F}\")\n",
    "print(f\"MAE: {mae_best:0.4F}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/features_with_aggregated_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=temporal_features + ['target_mood'])\n",
    "print(f\"Dataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[temporal_features].copy()\n",
    "y = df['target_mood'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in splitter.split(X_scaled, groups=df['id']):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = df['id'].iloc[train_idx].values\n",
    "\n",
    "print(f\" Train set: {X_train.shape[0]} samples\")\n",
    "print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "print(f\" Number of unique participants in train: {np.unique(train_groups).shape[0]}\")\n",
    "print(f\" Number of unique participants in test: {np.unique(df['id'].iloc[test_idx]).shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cv_iterator(X, y, groups, n_splits=3):\n",
    "    for i in range(n_splits):\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=i)\n",
    "        for train_idx, val_idx in splitter.split(X, groups=groups):\n",
    "            yield train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_y_pred = base_model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, base_y_pred)\n",
    "base_mae = mean_absolute_error(y_test, base_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Baseline Random Forest Performance:\")\n",
    "print(f\" MSE: {base_mse:.4f}\")\n",
    "print(f\" MAE: {base_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}# Parameter gird for Searching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = list(group_cv_iterator(X_train, y_train, train_groups, n_splits=3))\n",
    "\n",
    "# Create and fit GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=mse_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" GridSearchCV results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {-grid_search.best_score_:.4f} (MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_model = grid_search.best_estimator_\n",
    "grid_y_pred = best_grid_model.predict(X_test)\n",
    "grid_mse = mean_squared_error(y_test, grid_y_pred)\n",
    "grid_mae = mean_absolute_error(y_test, grid_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" GridSearchCV best model performance:\")\n",
    "print(f\" MSE: {grid_mse:.4f}\")\n",
    "print(f\" MAE: {grid_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
